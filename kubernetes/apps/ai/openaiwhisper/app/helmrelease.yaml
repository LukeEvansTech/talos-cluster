---
# yaml-language-server: $schema=https://raw.githubusercontent.com/bjw-s-labs/helm-charts/main/charts/other/app-template/schemas/helmrelease-helm-v2.schema.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: openaiwhisper
spec:
  interval: 1h
  chartRef:
    kind: OCIRepository
    name: openaiwhisper
  values:
    controllers:
      openaiwhisper:
        annotations:
          reloader.stakater.com/auto: "true"
        containers:
          app:
            image:
              repository: fedirz/faster-whisper-server
              tag: 0.5.0-cuda@sha256:0b64050ad0b9244745746b652473ee42a8d5454d501877a252c3e65f631ffc99
            env:
              # Default model - can be overridden per request
              WHISPER__MODEL: Systran/faster-whisper-large-v3
              # Idle timeout before unloading model (seconds)
              WHISPER__INFERENCE_DEVICE: cuda
              WHISPER__COMPUTE_TYPE: float16
            probes:
              liveness: &probes
                enabled: true
                custom: true
                spec:
                  httpGet:
                    path: /health
                    port: &port 8000
                  initialDelaySeconds: 30
                  periodSeconds: 30
                  timeoutSeconds: 10
                  failureThreshold: 3
              readiness: *probes
            resources:
              requests:
                nvidia.com/gpu: 1
                cpu: 500m
                memory: 4Gi
              limits:
                memory: 16Gi
                nvidia.com/gpu: 1
    defaultPodOptions:
      labels:
        gpu-workload: "true"
      runtimeClassName: nvidia
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: extensions.talos.dev/nvidia-open-gpu-kernel-modules-production
                    operator: Exists
                  - key: extensions.talos.dev/nvidia-container-toolkit-production
                    operator: Exists
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              gpu-workload: "true"
    service:
      app:
        controller: openaiwhisper
        ports:
          http:
            port: *port
    persistence:
      cache:
        existingClaim: "{{ .Release.Name }}"
        globalMounts:
          - path: /root/.cache/huggingface
